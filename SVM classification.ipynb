{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "(2400, 40)\n",
      "(2400,)\n",
      "Accuracy: 0.625\n"
     ]
    }
   ],
   "source": [
    "# Implement an SVM classifier with hinge loss\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import svm\n",
    "import os\n",
    "\n",
    "# Generate dataset\n",
    "file_path = [r\"animals/cats\", r\"animals/dogs\", r\"animals/panda\"]\n",
    "categories = {0: \"cat\", 1: \"dog\", 2: \"panda\"}\n",
    "def load_data():\n",
    "    data = []\n",
    "    for i in range(3):\n",
    "        for file in os.listdir(file_path[i]):\n",
    "            if file.endswith(\".jpg\"):\n",
    "                features = np.load(file_path[i] + \"/\" + file + \"_histogram.npy\")\n",
    "                data.append([None, i, file_path[i] + \"/\" + file, features])\n",
    "    return data\n",
    "data = load_data()\n",
    "print(len(data))\n",
    "\n",
    "features = np.array([x[3] for x in data])\n",
    "lables = np.array([x[1] for x in data])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, lables, test_size=0.2, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "# Train the SVM classifier\n",
    "svm_classifier = svm.SVC(kernel='linear')\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after PCA: 0.6183333333333333\n"
     ]
    }
   ],
   "source": [
    "# try to improve the accuracy by using PCA\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=30)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Train the SVM classifier\n",
    "svm_classifier = svm.SVC(kernel='linear')\n",
    "svm_classifier.fit(X_train_pca, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_classifier.predict(X_test_pca)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy after PCA: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  compare these two versions(simple and PCA).\n",
    "\n",
    "PCA is a method to reduce the dimension of the data. It is useful when the data has many features. In this case, we reduce the dimension of features from 128 to 30. And it has a loss for information of the data. So, we can compare the two versions to see the difference in the result.PCA is less accurate than the simple version. But it is faster than the simple version. So, we can use PCA when we need to reduce the time of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after using rbf kernel: 0.65\n"
     ]
    }
   ],
   "source": [
    "# try to improve the accuracy by using another kernel\n",
    "\n",
    "# Train the SVM classifier\n",
    "svm_classifier = svm.SVC(kernel='rbf')\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy after using rbf kernel: {accuracy}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the training of a Support Vector Machine (SVM) classifier, the backward pass involves updating the model parameters (weights and bias) using the gradients of the loss function with respect to these parameters.\n",
    "\n",
    "1. **Loss Function**: penalizes misclassifications (hinge loss) and a regularization term to control the complexity of the model.\n",
    "\n",
    "2. **Gradient Calculation**: The gradients of the loss function with respect to the model parameters (weights and bias) are calculated using techniques like gradient descent or more advanced optimization algorithms\n",
    "\n",
    "3. **Backward Pass**: During the backward pass, the gradients are propagated backward through the network using the chain rule of calculus to calculate how much each parameter contributed to the loss.\n",
    "\n",
    "4. **Parameter Update**: The model parameters are then updated in the opposite direction of the gradients to minimize the loss function. The size of the update is controlled by a parameter called the learning rate, which determines how much the parameters should be adjusted in each iteration.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
