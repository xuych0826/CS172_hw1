{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding\n",
    "\n",
    "we can drive the formula for the padding of a convolutional layer by considering the following:\n",
    "\n",
    "- \\(H\\): the height of the input\n",
    "- \\(F\\)：the height of the filter\n",
    "- \\(P\\)：the padding\n",
    "- \\(S\\)：the stride\n",
    "- \\(H'\\)：the height of the output\n",
    "\n",
    "The formula for calculating the height of the output is given by:\n",
    "\n",
    "```python\n",
    "H' = (H - F + 2P)//S + 1\n",
    "```\n",
    "\n",
    "Rearranging the formula, we get:\n",
    "padding of conv1 = 1\n",
    "padding of conv2 = 1\n",
    "padding of conv3 = 1\n",
    "padding of conv4 = 1\n",
    "padding of conv5 = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![D_loss_using_basic](D_loss_using_basic.png)\n",
    "![G_loss_using_basic](G_loss_using_basic.png)\n",
    "![D_loss_using_deluxe](D_loss_using_deluxe.png)\n",
    "![G_loss_using_deluxe](G_loss_using_deluxe.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **Generator Loss Curve**:\n",
    "   - Initially, the generator loss might be high as it learns to generate realistic samples.\n",
    "   - As training progresses, the generator loss should decrease, indicating that the generator is improving at generating realistic data.\n",
    "   - A well-trained generator will have a low and stable loss.\n",
    "\n",
    "2. **Discriminator Loss Curve**:\n",
    "   - The discriminator loss curve might start high as it learns to differentiate between real and generated samples.\n",
    "   - Over time, the discriminator loss should decrease as it becomes more challenging for the discriminator to distinguish between real and fake data.\n",
    "   - A well-trained discriminator will have a low and stable loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "early samples are not good, but as the training progresses, the samples become more realistic, the images are more clear and the face is more recognizable.\n",
    "\n",
    "early samples(iteration = 200):\n",
    "![real-000200](output\\vanilla\\grumpifyAprocessed_deluxe\\real-000200.png)  \n",
    "\n",
    "late samples(iteration = 3000):\n",
    "![real-003000](output\\vanilla\\grumpifyAprocessed_deluxe\\real-003000.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6\n",
    "\n",
    "## problems\n",
    " - DCGANs can still suffer from training instability, where the generator and discriminator struggle to reach a Nash equilibrium, leading to oscillations in the training process.\n",
    " - Mode collapse can still occur in DCGANs, where the generator collapses to producing a limited set of samples, lacking diversity in the generated outputs.\n",
    "\n",
    "## improvements\n",
    " yes, there are several improvements that can be made to DCGANs to address these issues:\n",
    " - introducing architectural modifications such as spectral normalization, self-attention mechanisms, skip connections, or progressive growing techniques can enhance the stability and performance of generative models.\n",
    " - Implementing advanced training strategies like Wasserstein GANs, spectral normalization, or feature matching can improve the training stability of GANs.\n",
    "\n",
    "regarding other generative models:\n",
    "\n",
    "- **VAE**: Variational Autoencoders can also face challenges such as posterior collapse, mode dropping, and blurry outputs. Techniques like annealed training schedules, variational annealing, and improved architectures can help address these issues.\n",
    "\n",
    "- **Diffusion Models**: Diffusion models are relatively new and have shown promising results in generating high-quality samples. However, challenges such as training stability, scalability to large datasets, and computational efficiency still need to be addressed through algorithmic advancements and architectural improvements."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
